# Image-Captioning
It is the implementation of a soft attention based Image Caption Generator. It is based on widely used Encoder-Decoder Framework, where the encoder is a Convolutional Neural Network (CNN) and the decoder has long short-term memory(LSTM) architecture. This repository uses [ResNet-50](https://arxiv.org/abs/1512.03385) model pretrained on [ILSVRC-2012](http://www.image-net.org/challenges/LSVRC/2012/) dataset for encoding. Soft attention mechanism is implemented to better understand the semantics of the images. The captions are generated using Beam-Search algorithm. This implementation is roughly based on the paper "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention" by Xu et al. (ICML2015).

<h1 style="font-size:10vw"> Usage </h1>

The model can be trained using t
```

```
